{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hristy26/news-articles/blob/main/news_articles_preprocessing_galvan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avTlNVAH1Z8V",
        "outputId": "d00e3bc8-7bee-4cf3-93f4-863ae56bb664"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.44.1)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.3.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.7)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.12)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.6.9)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.3)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.31.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (12.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi<1.0->gradio) (0.38.6)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Requirement already satisfied: sentence-transformers==2.2.2 in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.19.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.13.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.2.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.19.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers==2.2.2) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers==2.2.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers==2.2.2) (3.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers==2.2.2) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "#Import our dependencies\n",
        "!pip install gradio\n",
        "!pip install sentence-transformers==2.2.2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Models to use in our pipeline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1KdvOfNg1Z8X"
      },
      "outputs": [],
      "source": [
        "#Import and read news articles\n",
        "articles_df = pd.read_csv(\"news_articles.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7r52SvJ953lP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kw-ESDin1Z8X"
      },
      "outputs": [],
      "source": [
        "articles_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G89t2lUL1Z8X"
      },
      "outputs": [],
      "source": [
        "articles_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dk2njrR1Z8Y"
      },
      "outputs": [],
      "source": [
        "articles_df.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glIYcP4m1Z8Y"
      },
      "outputs": [],
      "source": [
        "articles_df[\"label\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Vmrun3N1Z8Y"
      },
      "outputs": [],
      "source": [
        "# Convert the \"title\" column from the news articles DataFrame to a list.\n",
        "title_list = articles_df[\"title\"].tolist()\n",
        "title_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmT-6q3W1Z8Y"
      },
      "outputs": [],
      "source": [
        "# Convert the \"text\" column from the news articles DataFrame to a list.\n",
        "text_list = articles_df[\"text\"].tolist()\n",
        "text_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlM3bbzi1Z8Y"
      },
      "outputs": [],
      "source": [
        "# Convert the \"title_without_stopwords\" column from the news articles DataFrame to a list.\n",
        "title_without_stopwords_list = articles_df[\"title_without_stopwords\"].tolist()\n",
        "title_without_stopwords_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVoZCzQ-1Z8Z"
      },
      "outputs": [],
      "source": [
        "# Convert the \"text_without_stopwords\" column from the news articles DataFrame to a list.\n",
        "text_without_stopwords_list = articles_df[\"text_without_stopwords\"].tolist()\n",
        "text_without_stopwords_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtPmUcGK1Z8Z"
      },
      "outputs": [],
      "source": [
        "#Create an instance of the label encoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "#Copy datafram\n",
        "encoded_articles_df = articles_df.copy()\n",
        "LabelEncoder().fit_transform\n",
        "\n",
        "# Fit and transform the label encoder for each column\n",
        "for column in encoded_articles_df:\n",
        "    encoded_articles_df[column] = le.fit_transform(encoded_articles_df[column])\n",
        "\n",
        "encoded_articles_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aR1CiTF1Z8Z"
      },
      "outputs": [],
      "source": [
        "encoded_articles_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovS-Rw181Z8a"
      },
      "outputs": [],
      "source": [
        "encoded_articles_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3cjD2DP1Z8a"
      },
      "outputs": [],
      "source": [
        "encoded_articles_df[\"label\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2K9w699i1Z8a"
      },
      "outputs": [],
      "source": [
        "encoded_articles_df[\"label\"].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mrn3ub9U1Z8a"
      },
      "outputs": [],
      "source": [
        "y_encoded_df = encoded_articles_df[\"label\"]\n",
        "y_encoded_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eFjUCZ81Z8a"
      },
      "outputs": [],
      "source": [
        "X_encoded_df = encoded_articles_df.drop([\"label\"], axis = 1)\n",
        "X_encoded_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKokVELW1Z8a"
      },
      "outputs": [],
      "source": [
        "#Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_encoded_df, y_encoded_df, random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sO9HNdJe1Z8a"
      },
      "outputs": [],
      "source": [
        "#Create the model\n",
        "model = LogisticRegression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJqyvDr71Z8a"
      },
      "outputs": [],
      "source": [
        "#Fit the model to the training data\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTQx7zMY1Z8a"
      },
      "outputs": [],
      "source": [
        "# Calculate the mean_squared_error and the r-squared value\n",
        "# for the testing data\n",
        "\n",
        "# Use our model to make predictions\n",
        "predicted = model.predict(X_test)\n",
        "\n",
        "# Score the predictions with mse and r2\n",
        "mse = mean_squared_error(y_test, predicted)\n",
        "r2 = r2_score(y_test, predicted)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, predicted))\n",
        "\n",
        "print(f\"mean squared error (MSE): {mse}\")\n",
        "print(f\"R-squared (R2): {r2}\")\n",
        "print(f\"Root mean squarted error (RMSE): {rmse}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUS5tUSj1Z8b"
      },
      "outputs": [],
      "source": [
        "# Call the `score()` method on the model to show the R2 score\n",
        "model.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6l_uLHZ1Z8b"
      },
      "outputs": [],
      "source": [
        "def text_classification(articles_df):\n",
        "\n",
        "    # Set the features variable to the title message column.\n",
        "    articles_df = articles_df.dropna()\n",
        "    X = articles_df['text']\n",
        "\n",
        "    # Set the target variable to the \"label\" column.\n",
        "    y = articles_df['label']\n",
        "\n",
        "    # Split data into training and testing and set the test_size = 33%\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "    print(y_train.info())\n",
        "\n",
        "    # Build a pipeline to transform the test set to compare to the training set.\n",
        "    text_classification = Pipeline([('tfidf', TfidfVectorizer(stop_words='english')),\n",
        "                     ('clf', LinearSVC()),\n",
        "])\n",
        "\n",
        "    # Fit the model to the transformed training data and return model.\n",
        "\n",
        "    model = text_classification.fit(X_train, y_train)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c9E9KCu1Z8b"
      },
      "outputs": [],
      "source": [
        "# Call the title_classification function with the DataFrame and set the result to the \"title_clf\" variable\n",
        "text_classification = text_classification(articles_df)\n",
        "text_classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xa80IpfB1Z8b"
      },
      "outputs": [],
      "source": [
        "# Create a function called `text_prediction` that takes in the text and predicts the whether the text is \"fake\" or \"real\".\n",
        "# The function should return the Text, and say whether the text is \"fake\" or \"real\".\n",
        "def text_prediction(text):\n",
        "\n",
        "    # Create a variable that will hold the prediction of a new text.\n",
        "    text_predictions = text_classification.predict([text])\n",
        "\n",
        "    # Using a conditional if the prediction is \"real\" return the message:\n",
        "    # f'The text message: \"{text}\", is fake.' Else, return f'The text message: \"{text}\", is real.'\n",
        "\n",
        "    if text_predictions[0] == 'real':\n",
        "        return f'The text: \"{text}\", is real.'\n",
        "    else:\n",
        "        return f'The text: \"{text}\", is fake.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3buSwz151Z8b"
      },
      "outputs": [],
      "source": [
        "articles_df['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65HlXGDv1Z8b"
      },
      "outputs": [],
      "source": [
        "# Create a title_app that takes a textbox for the inputs and has a textbox for the output.\n",
        "# Povide labels for each textbox.\n",
        "\n",
        "app = gr.Interface(\n",
        "        fn=text_prediction,\n",
        "inputs = [\n",
        "gr.Textbox(label=\"What is the text you want to test?\")],\n",
        "outputs=gr.Textbox(lines=10, label=\"Our app has determined: \", show_copy_button=True))\n",
        "\n",
        "\n",
        "# Launch the app.\n",
        "#app.launch(show_error=True)\n",
        "\n",
        "app.launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SVkPuAT1Z8b"
      },
      "outputs": [],
      "source": [
        "# Create the random forest classifier model\n",
        "\n",
        "\n",
        "#randomforest_model = RandomForestClassifier(n_estimators=128, random_state=1)\n",
        "randomforest_model = RandomForestClassifier(max_depth=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmMFORqS1Z8c"
      },
      "outputs": [],
      "source": [
        "# Fit the model to the training data\n",
        "randomforest_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJCD_Zgv1Z8c"
      },
      "outputs": [],
      "source": [
        "# Validate the model by checking the model accuracy with model.score\n",
        "print(f\"Training Data Score: {randomforest_model.score(X_train, y_train)}\")\n",
        "print(f\"Testing Data Score: {randomforest_model.score(X_test, y_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpB1IXzJ1Z8c"
      },
      "outputs": [],
      "source": [
        "# Make predictions and produce the classification report for the randome forest model\n",
        "predictions = randomforest_model.predict(X_test)\n",
        "print(classification_report(y_test, predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fs0R59W1Z8c"
      },
      "outputs": [],
      "source": [
        "articles_df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTJ2hBPx1Z8c"
      },
      "outputs": [],
      "source": [
        "def read_process(articles_df, features, target):\n",
        "\n",
        "    # Drop missing values\n",
        "    articles_df = articles_df.dropna()\n",
        "    X = articles_df[features]\n",
        "    y = articles_df[target]\n",
        "\n",
        "    # Check for categorical variables\n",
        "    categorical_columns = X.select_dtypes(include=['object', 'category']).columns\n",
        "    numerical_columns = X.select_dtypes(exclude=['object', 'category']).columns\n",
        "\n",
        "    # Handle categorical columns variables (if needed)\n",
        "    X = pd.get_dummies(X, columns=categorical_columns, dtype=float)  # Binary classification assumed here\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    y_encoded = le.fit_transform(y)\n",
        "\n",
        "    # # Return data with both numerical and categorical columns separated\n",
        "    # return X, y, numerical_columns, categorical_columns\n",
        "    return X, y_encoded\n",
        "\n",
        "def model_generator(articles_df, features, target):\n",
        "    X, y = read_process(articles_df, features, target)\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "    # Column transformer to handle scaling and encoding\n",
        "    # preprocessor = ColumnTransformer(\n",
        "    #     transformers=[\n",
        "    #         ('num', StandardScaler(), numerical_columns),\n",
        "    #         ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns)\n",
        "    #     ]\n",
        "    # )\n",
        "\n",
        "\n",
        "    models = {\n",
        "        \"Logistic Regression\": LogisticRegression(),\n",
        "        \"SVR\": SVR(),\n",
        "        \"Random Forest\": RandomForestClassifier(),\n",
        "        \"Gradient Boosting\": GradientBoostingClassifier(),\n",
        "        \"Decision Tree\": DecisionTreeClassifier()\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "\n",
        "\n",
        "\n",
        "    for name, model in models.items():\n",
        "        pipeline = Pipeline([\n",
        "            (\"scale\", StandardScaler()),  # Apply preprocessor to handle encoding and scaling\n",
        "            (name, model)\n",
        "        ])\n",
        "\n",
        "        pipeline.fit(X_train, y_train)\n",
        "        y_predictions = pipeline.predict(X_test).reshape(-1, 1)\n",
        "        score = pipeline.score(X_test, y_test)\n",
        "        results[name] = score\n",
        "        print(f\"{name} Score: {score}\")\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ik7svQXf1Z8c"
      },
      "outputs": [],
      "source": [
        "features = articles_df.drop([\"label\", \"type\"], axis=1).columns.to_list()\n",
        "target = \"label\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yDic6KV1Z8c"
      },
      "outputs": [],
      "source": [
        "articles_df['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCv5rK811Z8d"
      },
      "outputs": [],
      "source": [
        "read_process(articles_df, features, target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aURSJK41Z8d"
      },
      "outputs": [],
      "source": [
        "#Generate accuracy score by model\n",
        "model_generator(articles_df, features, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz3x1pIR1Z8d"
      },
      "source": [
        "OPTIMIZATION - HYPERPARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIhGIbA41Z8d"
      },
      "outputs": [],
      "source": [
        "# Create KNN classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "random_tuned_model = KNeighborsClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fv2NRIgk1Z8r"
      },
      "outputs": [],
      "source": [
        "# Create the parameter object for the randomized search estimator.\n",
        "# Try adjusting n_neighbors with values of 1 through 19.\n",
        "# Adjust leaf_size by using a range from 1 to 500.\n",
        "# Include both uniform and distance options for weights.\n",
        "\n",
        "param_grid = {\n",
        "     'n_neighbors': np.arange(1,20,2),\n",
        "     'weights': ['uniform', 'distance'],\n",
        "     'leaf_size': np.arange(1, 500)\n",
        " }\n",
        "param_grid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gLoW3Bv1Z8r"
      },
      "outputs": [],
      "source": [
        "# Create the randomized search estimator\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "random_clf = RandomizedSearchCV(random_tuned_model, param_grid, random_state=0, verbose=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Dd4CW191Z8s"
      },
      "outputs": [],
      "source": [
        "# Fit the model by using the randomized search estimator.\n",
        "random_clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrekbGsF1Z8s"
      },
      "outputs": [],
      "source": [
        "# List the best parameters for this dataset\n",
        "print(random_clf.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jPaCLzy1Z8s"
      },
      "outputs": [],
      "source": [
        "# Print the classification report for the best model\n",
        "grid_y_pred = random_clf.predict(X_test)\n",
        "print(classification_report(y_test, grid_y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhgGg_W51Z8s"
      },
      "outputs": [],
      "source": [
        "# Make predictions with the hypertuned model\n",
        "random_tuned_pred = random_clf.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FTN_rWo1Z8s"
      },
      "outputs": [],
      "source": [
        "# Calculate the classification report\n",
        "print(classification_report(y_test, random_tuned_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9i0KRnie1Z8s"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dev",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}